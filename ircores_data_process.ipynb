{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3332\n",
      "How did England and Scotland become united\n"
     ]
    }
   ],
   "source": [
    "path = 'train.tsv'\n",
    "path_p = 'document_passages.json'\n",
    "question = []\n",
    "passage = []\n",
    "# question\n",
    "counter = 0\n",
    "line_ori = []\n",
    "QID = []\n",
    "with open(path,'r') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        line_ori.append(line)\n",
    "        counter += 1\n",
    "        if len(line) != 0:\n",
    "            # delete all characters\n",
    "            question.append(re.sub('\\W',\" \",line.split('\\t')[1].strip('?')))\n",
    "            QID.append(line.split('\\t')[0])\n",
    "        if not line:\n",
    "            break\n",
    "        pass #do something\n",
    "# the first row is all for titles, delete that one\n",
    "del question[0]\n",
    "del QID[0]\n",
    "print(type(question))\n",
    "print(len(question))\n",
    "print(question[415])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "416\n",
      "Why are drainage basins important to the ecology\n"
     ]
    }
   ],
   "source": [
    "path = 'test.tsv'\n",
    "path_p = 'document_passages.json'\n",
    "question = []\n",
    "passage = []\n",
    "# question\n",
    "counter = 0\n",
    "line_ori = []\n",
    "QID = []\n",
    "with open(path,'r') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        line_ori.append(line)\n",
    "        counter += 1\n",
    "        if len(line) != 0:\n",
    "            # delete all characters\n",
    "            question.append(re.sub('\\W',\" \",line.split('\\t')[1].strip('?')))\n",
    "            QID.append(line.split('\\t')[0])\n",
    "        if not line:\n",
    "            break\n",
    "        pass #do something\n",
    "# the first row is all for titles, delete that one\n",
    "del question[0]\n",
    "del QID[0]\n",
    "print(type(question))\n",
    "print(len(question))\n",
    "print(question[415])\n",
    "\n",
    "\n",
    "# corpus\n",
    "with open(path_p,'r') as f:\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        passage.append(line)\n",
    "        if not line:\n",
    "            break\n",
    "        pass #do something\n",
    "    \n",
    "passage_dic = {}# maybe used for future work\n",
    "passage_ls = [] # used for input of TfidfVectorizer or CountVectorizer\n",
    "for i in range(len(passage[0].split('\"}, \"'))):\n",
    "    # passage for one article\n",
    "    passage_ls.append(passage[0].split('\"}, \"')[i].split(': {\"')[1])\n",
    "    article_p = passage[0].split('\"}, \"')[i].split(': {\"')[1].split('\", \"')\n",
    "    # indice of article \n",
    "    passage_dic[re.sub('\\D',\"\",passage[0].split('\"}, \"')[i].split(': {\"')[0])] = article_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract(psg):\n",
    "    doc_id = psg.split('\": {\"')[0]\n",
    "#         doc_id = psg.split('\": {\"')[0]\n",
    "    for j in range(len(psg.split('\": {\"')[1].split('\", \"'))):\n",
    "        psg_id = psg.split('\": {\"')[1].split('\", \"')[j].split('\": \"')[0]\n",
    "        psg_con = psg.split('\": {\"')[1].split('\", \"')[j].split('\": \"')[1]\n",
    "        \n",
    "        psg_tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(psg_con) if word.lower() not in  new]\n",
    "        psg_tokens_f = (' '.join(psg_tokens))\n",
    "        \n",
    "        new_psg_id = '-'.join([doc_id,psg_id])\n",
    "        f_id.append(new_psg_id)\n",
    "        # build a trectext file\n",
    "        trec.append('<DOC>\\n' + \n",
    "                '<DOCNO>' + new_psg_id + '</DOCNO>\\n' +\n",
    "                '<TEXT>' + psg_tokens_f + '</TEXT>\\n' + \n",
    "                '</DOC>\\n')\n",
    "    return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage processing\n",
    "trec = []\n",
    "f_id = []\n",
    "for i in range(len(passage[0].split('\"}, \"'))):\n",
    "# for i in range(4):\n",
    "    psg = passage[0].split('\"}, \"')[i]\n",
    "    # extra processing for the first doc and the last doc.\n",
    "    if i == 0:\n",
    "        psg = passage[0].split('\"}, \"')[i].strip('{\"')\n",
    "    if i == 862:\n",
    "        psg = passage[0].split('\"}, \"')[i].strip('\"}}')\n",
    "    # extract document id from the first snippet.\n",
    "    extract(psg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "corpus = open('corpus_trectext_new_pre.txt', 'w')\n",
    "for p in trec:\n",
    "    corpus.write(p)\n",
    "    corpus.write('\\n')\n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the role of conversionism in Evangelicalism\n",
      "<class 'str'>\n",
      "role conversionism Evangelicalism\n"
     ]
    }
   ],
   "source": [
    "# pre-processing for query\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "more = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "new_stop = stopwords + more\n",
    "count = 0\n",
    "q_tokens_f=[]\n",
    "for i in range(len(question)):\n",
    "    q_tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(question[i]) if word.lower() not in  new_stop]\n",
    "#     q_tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(question[i])]\n",
    "    q_tokens_f.append(' '.join(q_tokens))\n",
    "    count+=1\n",
    "print(question[0])\n",
    "print(type(q_tokens_f[0]))\n",
    "print(q_tokens_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3332\n",
      "What is the role of conversionism in Evangelicalism\n"
     ]
    }
   ],
   "source": [
    "print(type(q_tokens_f))\n",
    "print(len(q_tokens_f))\n",
    "print(q_tokens_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build query_file\n",
    "query = open('query_train_st_tk_sw.txt','w')\n",
    "for q in range(len(question)):\n",
    "#     q_trectext =( '<query>\\n'+\n",
    "#                 '<type>'+ 'indri'+'</type>\\n'+\n",
    "#                 '<number>'+ QID[q] + '</number>\\n'+\n",
    "#                 '<text>' + question[q] + '</text>\\n'+\n",
    "#                 '</query>\\n')\n",
    "    if q == 0:\n",
    "        q_trectext = ('<parameters>\\n' +\n",
    "        '<query>\\n'+\n",
    "        '<type>'+ 'indri'+'</type>\\n'+\n",
    "        '<number>'+ QID[q] + '</number>\\n'+\n",
    "        '<text>' + q_tokens_f[q] + '</text>\\n'+\n",
    "        '</query>\\n')\n",
    "    else:\n",
    "        if q == (len(question)-1):\n",
    "            q_trectext = ('<query>\\n'+\n",
    "            '<type>'+ 'indri'+'</type>\\n'+\n",
    "            '<number>'+ QID[q] + '</number>\\n'+\n",
    "            '<text>' + q_tokens_f[q] + '</text>\\n'+\n",
    "            '</query>\\n' +\n",
    "            '</parameters>\\n')\n",
    "        else:\n",
    "            q_trectext =( '<query>\\n'+\n",
    "                '<type>'+ 'indri'+'</type>\\n'+\n",
    "                '<number>'+ QID[q] + '</number>\\n'+\n",
    "                '<text>' + q_tokens_f[q] + '</text>\\n'+\n",
    "                '</query>\\n')\n",
    "    query.write(q_trectext)\n",
    "query.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure query list, ignore it\n",
    "queryfile = open('query.txt','w')\n",
    "for query in question:\n",
    "    queryfile.write(query)\n",
    "    queryfile.write('\\n')\n",
    "queryfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qrels file: OPIC      ITERATION      DOCUMENT#      RELEVANCY \n",
    "new=line_ori[1:3334]\n",
    "qrels = open('qrels_test.txt','w')\n",
    "# each query only have one relevant document with maybe one or several passage\n",
    "for q in range(len(QID)):\n",
    "    rlv_p = new[q].split('\\t')[4].split('\\n')[0].split(',')\n",
    "    rlv_d = new[q].split('\\t')[2]\n",
    "    qid = QID[q]\n",
    "    for p in rlv_p:\n",
    "        rlv_id = '-'.join([rlv_d,p])\n",
    "        qrels_ls = (qid + 'Q0'+ rlv_id + '1\\n' )\n",
    "        qrels.write(qid)\n",
    "        qrels.write(' ')\n",
    "        qrels.write('0')\n",
    "        qrels.write(' ')\n",
    "        qrels.write(rlv_id)\n",
    "        qrels.write(' ')\n",
    "        qrels.write('1\\n')\n",
    "qrels.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
